{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/astrocyte/Accelerate/blob/master/MCSS_CR_Kmeans_Global_Centroids.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_dest_folder = \"/content/drive/MyDrive/Shawn_buffer_1/K-Means Run Global Centroids\"  #<<********* Modify this base path as needed!!!!!!!!!!!!!!!"
      ],
      "metadata": {
        "id": "2KbTjVZTiM_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8aceb9d-14ca-43fb-f963-647e85c92e8d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI21nOdCiBLT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "\n",
        "from scipy.linalg import norm, toeplitz\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Set seed for reproducibility\n",
        "seed = 123\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Parameters\n",
        "dim = 100\n",
        "n = 10000\n",
        "variance_setting = \"base\"\n",
        "covariance_type = \"toeplitz\"  # Can be \"diagonal or \"toeplitz\"\n",
        "B = 200  # Number of Monte Carlo iterations\n",
        "n_clusters_1 = 4\n",
        "percent_train = 0.8\n",
        "train_num_1 = int(n * percent_train)\n",
        "test_num_1 = n - train_num_1\n",
        "\n",
        "# Paths\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "save_folder = f\"GaussMix4_{n}_n_clusters{n_clusters_1}_samples_{dim}_dim_{variance_setting}_variance_{covariance_type}_covariance_{timestamp}\"\n",
        "data_dir = os.path.join(save_folder, 'data')\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "samples_full_path = os.path.join(data_dir, 'samples_original_1.csv')\n",
        "gt_labels_full_path = os.path.join(data_dir, 'ground_truth_labels_1.csv')\n",
        "\n",
        "mc_subsets_dir = os.path.join(save_folder, 'monte_carlo_subsets')\n",
        "\n",
        "# Gaussian Mixture Parameters\n",
        "mean_vectors = [\n",
        "    np.ones(dim),\n",
        "    4 * np.ones(dim),\n",
        "    7 * np.ones(dim),\n",
        "    10 * np.ones(dim)\n",
        "]\n",
        "weights = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "# Setting Variance Scale based on variance_setting parameters\n",
        "if variance_setting == 'low':\n",
        "    variance_scale = 0.1\n",
        "elif variance_setting == 'base':\n",
        "    variance_scale = 1\n",
        "elif variance_setting == \"high\":\n",
        "    variance_scale = 10\n",
        "\n",
        "# Setting covariance matrix based on selection\n",
        "def create_covariance_matrix(dim, variance_scale, covariance_type):\n",
        "    if covariance_type == \"diagonal\":\n",
        "        return variance_scale * np.identity(dim)\n",
        "    elif covariance_type == \"toeplitz\":\n",
        "        set_Toeplitz_part1 = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.001]\n",
        "        set_Toeplitz_part2 = [1, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.001]\n",
        "\n",
        "        set_Toeplitz_part1 += [0] * (dim - len(set_Toeplitz_part1))\n",
        "        set_Toeplitz_part2 += [0] * (dim - len(set_Toeplitz_part2))\n",
        "\n",
        "        Toeplitz_mat = toeplitz(set_Toeplitz_part1, set_Toeplitz_part2)\n",
        "\n",
        "        # Scale the diagonal by the constant (variance_scale) if variance is to be adjusted\n",
        "        Toeplitz_mat = np.diag(np.diag(Toeplitz_mat) * variance_scale) + Toeplitz_mat - np.diag(np.diag(Toeplitz_mat))\n",
        "\n",
        "        return Toeplitz_mat\n",
        "    else:\n",
        "        raise ValueError(\"Invalid covariance_type. Use 'diagonal' or 'toeplitz'.\")\n",
        "\n",
        "covariance_matrices = [create_covariance_matrix(dim, variance_scale, covariance_type) for _ in range(4)]\n",
        "\n",
        "def generate_samples(n, dim, mean_vectors, covariance_matrices, weights):\n",
        "    samples = np.empty((n, dim))\n",
        "    component_list = np.empty((n, 1))\n",
        "    for i in range(n):\n",
        "        component = random.choices(range(len(weights)), weights=weights)[0]\n",
        "        sample = np.random.multivariate_normal(mean_vectors[component], covariance_matrices[component])\n",
        "        samples[i, :] = sample\n",
        "        component_list[i] = component\n",
        "    return samples, component_list\n",
        "\n",
        "def report_fractions(component_list, weights):\n",
        "    for i in range(len(weights)):\n",
        "        print(f'Fraction of mixture component {i}: {np.sum(component_list == i) / n}')\n",
        "\n",
        "def visualize_samples(samples):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(121)\n",
        "    plt.plot(samples[:, 0], samples[:, 1], '.', alpha=0.5)\n",
        "    plt.title('First two dimensions')\n",
        "    plt.grid()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(samples[:, 0], samples[:, -1], '.', alpha=0.5)\n",
        "    plt.title('First and last dimensions')\n",
        "    plt.grid()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def save_samples(samples, component_list, samples_path, gt_labels_path):\n",
        "    os.makedirs(os.path.dirname(samples_path), exist_ok=True)\n",
        "    pd.DataFrame(samples).to_csv(samples_path, index=False, header=False)\n",
        "    pd.DataFrame(component_list).to_csv(gt_labels_path, index=False, header=False)\n",
        "    print(f\"Samples saved to {samples_path}\")\n",
        "    print(f\"Ground truth labels saved to {gt_labels_path}\")\n",
        "\n",
        "def monte_carlo_subsampling(samples, n, B, subset1_size, save_folder):\n",
        "    \"\"\"\n",
        "    Perform repeated random splits of the data into two subsets.\n",
        "    These subsets will be used later for assessing clustering replicability.\n",
        "    Saves the indices of each split to allow reproducibility and further analysis.\n",
        "    \"\"\"\n",
        "    all_subset1_indices = []\n",
        "    all_subset2_indices = []\n",
        "    all_subset1_data = []\n",
        "    all_subset2_data = []\n",
        "\n",
        "    mc_subsets_dir = os.path.join(save_folder, 'monte_carlo_subsets')\n",
        "    os.makedirs(mc_subsets_dir, exist_ok=True)\n",
        "\n",
        "    for B_idx in range(B):\n",
        "        indices = random.sample(range(n), n)\n",
        "        subset1_indices = indices[:subset1_size]\n",
        "        subset2_indices = indices[subset1_size:]\n",
        "        all_subset1_indices.append(subset1_indices)\n",
        "        all_subset2_indices.append(subset2_indices)\n",
        "\n",
        "        subset1_data = samples[subset1_indices]\n",
        "        subset2_data = samples[subset2_indices]\n",
        "        all_subset1_data.append(subset1_data)\n",
        "        all_subset2_data.append(subset2_data)\n",
        "\n",
        "        subset1_indices_path = os.path.join(mc_subsets_dir, f'subset1_indices_{B_idx}.csv')\n",
        "        subset2_indices_path = os.path.join(mc_subsets_dir, f'subset2_indices_{B_idx}.csv')\n",
        "        pd.DataFrame(subset1_indices).to_csv(subset1_indices_path, index=False, header=False)\n",
        "        pd.DataFrame(subset2_indices).to_csv(subset2_indices_path, index=False, header=False)\n",
        "\n",
        "        subset1_data_path = os.path.join(mc_subsets_dir, f'subset1_data_{B_idx}.csv')\n",
        "        subset2_data_path = os.path.join(mc_subsets_dir, f'subset2_data_{B_idx}.csv')\n",
        "        pd.DataFrame(subset1_data).to_csv(subset1_data_path, index=False, header=False)\n",
        "        pd.DataFrame(subset2_data).to_csv(subset2_data_path, index=False, header=False)\n",
        "\n",
        "    return all_subset1_indices, all_subset2_indices, all_subset1_data, all_subset2_data\n",
        "\n",
        "def predict_ac(ac, train_data, test_data):\n",
        "    \"\"\"Custom predict function for Agglomerative Clustering\"\"\"\n",
        "    # Calculate centroids of training clusters\n",
        "    centroids = []\n",
        "    for i in range(ac.n_clusters_):\n",
        "        cluster_points = train_data[ac.labels_ == i]\n",
        "        centroid = np.mean(cluster_points, axis=0)\n",
        "        centroids.append(centroid)\n",
        "    centroids = np.array(centroids)\n",
        "\n",
        "    # Assign test points to nearest centroid\n",
        "    distances = cdist(test_data, centroids)\n",
        "    predictions = np.argmin(distances, axis=1)\n",
        "    return predictions\n",
        "\n",
        "def calculate_and_save_sorted_centroids(samples, n_clusters_1, save_folder):\n",
        "    \"\"\"Calculate centroids on full dataset and sort them by L2 norm\"\"\"\n",
        "    # Fit Agglomerative Clustering on the entire dataset\n",
        "    ac = AgglomerativeClustering(n_clusters=n_clusters_1, linkage=\"ward\").fit(samples)\n",
        "\n",
        "    # Calculate centroids for each cluster\n",
        "    centroid_values = []\n",
        "    for i in range(n_clusters_1):\n",
        "        cluster_points = samples[ac.labels_ == i]\n",
        "        centroid = np.mean(cluster_points, axis=0)\n",
        "        centroid_values.append(centroid)\n",
        "    centroid_values = np.array(centroid_values)\n",
        "\n",
        "    # Compute L2 norms and sort centroids\n",
        "    l2_norms = [np.linalg.norm(row) for row in centroid_values]\n",
        "    rows_with_norms = list(zip(centroid_values, l2_norms))\n",
        "    centroid_values_sorted = [row for row, norm in sorted(rows_with_norms, key=lambda x: x[1])]\n",
        "\n",
        "    # Save both original and sorted centroids\n",
        "    ac_folder = os.path.join(save_folder, 'ac_results', f'AC_{n_clusters_1}_clusters')\n",
        "    os.makedirs(ac_folder, exist_ok=True)\n",
        "\n",
        "    # Save original centers\n",
        "    centers_path = os.path.join(ac_folder, 'cluster_centers.csv')\n",
        "    pd.DataFrame(centroid_values).to_csv(centers_path, index=False, header=False)\n",
        "\n",
        "    # Save sorted centers\n",
        "    sorted_centroids_path = os.path.join(ac_folder, 'centroid_values_sorted.csv')\n",
        "    pd.DataFrame(centroid_values_sorted).to_csv(sorted_centroids_path, index=False, header=False)\n",
        "\n",
        "    return np.array(centroid_values_sorted)\n",
        "\n",
        "def perform_ac_clustering(all_train_data, all_test_data, B, n_clusters_1, save_folder):\n",
        "    ac_predicted_matrix = np.zeros((B, len(all_test_data[0])))\n",
        "\n",
        "    ac_folder = os.path.join(save_folder, 'ac_results', f'AC_{n_clusters_1}_clusters')\n",
        "    os.makedirs(ac_folder, exist_ok=True)\n",
        "\n",
        "    # Create type-specific folders\n",
        "    ac_cluster_centers_folder = os.path.join(ac_folder, 'ac_cluster_centers')\n",
        "    os.makedirs(ac_cluster_centers_folder, exist_ok=True)\n",
        "\n",
        "    matrix_data_cluster_folder = os.path.join(ac_folder, 'matrix_data_cluster')\n",
        "    os.makedirs(matrix_data_cluster_folder, exist_ok=True)\n",
        "\n",
        "    for B_idx in range(B):\n",
        "        train_data = all_train_data[B_idx]\n",
        "        test_data = all_test_data[B_idx]\n",
        "\n",
        "        ac = AgglomerativeClustering(n_clusters=n_clusters_1, linkage=\"ward\").fit(train_data)\n",
        "\n",
        "        # Calculate centroids and predict\n",
        "        centroids = []\n",
        "        for i in range(n_clusters_1):\n",
        "            cluster_points = train_data[ac.labels_ == i]\n",
        "            centroid = np.mean(cluster_points, axis=0)\n",
        "            centroids.append(centroid)\n",
        "        centroids = np.array(centroids)\n",
        "\n",
        "        # Predict using nearest centroid\n",
        "        predictions = predict_ac(ac, train_data, test_data)\n",
        "        ac_predicted_matrix[B_idx, :] = predictions\n",
        "\n",
        "        # Save files in type-specific folders\n",
        "        cluster_centers_path = os.path.join(ac_cluster_centers_folder, f\"ac_cluster_centers_{B_idx}.csv\")\n",
        "        pd.DataFrame(centroids).to_csv(cluster_centers_path, index=False, header=False)\n",
        "\n",
        "        matrix_data_cluster_path = os.path.join(matrix_data_cluster_folder, f\"matrix_data_cluster_{B_idx}.csv\")\n",
        "        pd.DataFrame(predictions).to_csv(matrix_data_cluster_path, index=False, header=False)\n",
        "\n",
        "    # Save ac_predicted_testdata_80_20_1.csv\n",
        "    ac_predicted_file_path = os.path.join(ac_folder, 'ac_predicted_testdata_80_20_1.csv')\n",
        "    pd.DataFrame(ac_predicted_matrix).to_csv(ac_predicted_file_path, index=False, header=False)\n",
        "    print(f'AC predicted test data saved to {ac_predicted_file_path}')\n",
        "\n",
        "    return ac_predicted_matrix\n",
        "\n",
        "def align_cluster_identities(B, test_num_1, n_clusters_1, save_folder):\n",
        "    \"\"\"\n",
        "    Aligns cluster identities across MCSS runs by comparing to global centroids.\n",
        "    Uses the full dataset centroids as reference points for alignment.\n",
        "    \"\"\"\n",
        "    form_80_20_matrix = np.zeros((B, test_num_1))\n",
        "    ac_folder = os.path.join(save_folder, 'ac_results', f'AC_{n_clusters_1}_clusters')\n",
        "\n",
        "    # Load global centroids (sorted)\n",
        "    sorted_centroids_path = os.path.join(ac_folder, 'centroid_values_sorted.csv')\n",
        "    whole_data_cluster_cent_1 = pd.read_csv(sorted_centroids_path, header=None).values\n",
        "\n",
        "    # Create aligned_matrix_data_cluster folder\n",
        "    aligned_matrix_data_cluster_folder = os.path.join(ac_folder, 'aligned_matrix_data_cluster')\n",
        "    os.makedirs(aligned_matrix_data_cluster_folder, exist_ok=True)\n",
        "\n",
        "    for B_idx in range(B):\n",
        "        print(f\"B_idx_1 index = {B_idx}\")\n",
        "\n",
        "        # Load cluster centers from type-specific folder\n",
        "        ac_cluster_centers_folder = os.path.join(ac_folder, 'ac_cluster_centers')\n",
        "        cluster_centers_path = os.path.join(ac_cluster_centers_folder, f'ac_cluster_centers_{B_idx}.csv')\n",
        "        matrix_data_idx = pd.read_csv(cluster_centers_path, header=None).values\n",
        "\n",
        "        # Compute distance matrix between bootstrap centroids and global centroids\n",
        "        hold_row_norm = np.zeros((n_clusters_1, n_clusters_1))\n",
        "        for i in range(n_clusters_1):\n",
        "            for j in range(n_clusters_1):\n",
        "                # Compute L2 distance between bootstrap centroid i and global centroid j\n",
        "                hold_row_norm[i,j] = np.sqrt(np.sum((matrix_data_idx[i] - whole_data_cluster_cent_1[j])**2))\n",
        "\n",
        "        # Initialize mapping arrays\n",
        "        sort_hold_row_norm = np.zeros(n_clusters_1, dtype=int)\n",
        "        used_indices = np.zeros(n_clusters_1, dtype=bool)\n",
        "\n",
        "        # Find optimal mapping ensuring each index is used only once\n",
        "        # Maps each bootstrap centroid to closest available global centroid\n",
        "        for i in range(n_clusters_1):\n",
        "            available_values = hold_row_norm[i, ~used_indices]\n",
        "            min_idx = np.argmin(available_values)\n",
        "            global_idx = np.where(~used_indices)[0][min_idx]\n",
        "            sort_hold_row_norm[i] = global_idx\n",
        "            used_indices[global_idx] = True\n",
        "\n",
        "        # Load cluster assignments\n",
        "        matrix_data_cluster_folder = os.path.join(ac_folder, 'matrix_data_cluster')\n",
        "        assignments_path = os.path.join(matrix_data_cluster_folder, f'matrix_data_cluster_{B_idx}.csv')\n",
        "        vector_to_be_arranged_1 = pd.read_csv(assignments_path, header=None).values.flatten()\n",
        "\n",
        "        # Transform assignments using mapping to global centroid indices\n",
        "        transformed_vector_1 = np.zeros(test_num_1)\n",
        "        for test_point_idx in range(test_num_1):\n",
        "            cluster_idx = int(vector_to_be_arranged_1[test_point_idx])\n",
        "            transformed_vector_1[test_point_idx] = sort_hold_row_norm[cluster_idx]\n",
        "\n",
        "        form_80_20_matrix[B_idx] = transformed_vector_1\n",
        "\n",
        "        # Save individual aligned assignments\n",
        "        aligned_path = os.path.join(aligned_matrix_data_cluster_folder, f'aligned_matrix_data_cluster_{B_idx}.csv')\n",
        "        pd.DataFrame(transformed_vector_1).to_csv(aligned_path, index=False, header=False)\n",
        "\n",
        "    # Save the complete aligned matrix\n",
        "    aligned_matrix_path = os.path.join(ac_folder, 'ac_predicted_testdata_80_20_1_aligned.csv')\n",
        "    pd.DataFrame(form_80_20_matrix).to_csv(aligned_matrix_path, index=False, header=False)\n",
        "    print(f\"Aligned matrix saved to {aligned_matrix_path}\")\n",
        "\n",
        "    return form_80_20_matrix\n",
        "\n",
        "def generate_and_save_clam_matrix(samples, all_test_indices, aligned_matrix, save_folder, n_clusters_1):\n",
        "    n = samples.shape[0]\n",
        "    clam_matrix = np.zeros((n, n_clusters_1))\n",
        "\n",
        "    ac_folder = os.path.join(save_folder, 'ac_results', f'AC_{n_clusters_1}_clusters')\n",
        "    os.makedirs(ac_folder, exist_ok=True)\n",
        "\n",
        "    # Create subfolder for individual CLAM matrices\n",
        "    clam_matrices_folder = os.path.join(ac_folder, 'clam_matrices')\n",
        "    os.makedirs(clam_matrices_folder, exist_ok=True)\n",
        "\n",
        "    for B_idx in range(aligned_matrix.shape[0]):\n",
        "        test_indices = all_test_indices[B_idx]\n",
        "        for i, sample_idx in enumerate(test_indices):\n",
        "            cluster_id = aligned_matrix[B_idx, i]\n",
        "            clam_matrix[sample_idx, int(cluster_id)] += 1\n",
        "\n",
        "        # Save individual CLAM matrix for each Monte Carlo iteration\n",
        "        clam_filename = f'clam_{B_idx}.csv'\n",
        "        clam_path = os.path.join(clam_matrices_folder, clam_filename)\n",
        "        pd.DataFrame(clam_matrix).to_csv(clam_path, index=False)\n",
        "        print(f\"CLAM matrix for iteration {B_idx} saved to {clam_path}\")\n",
        "\n",
        "    # Save the final CLAM matrix\n",
        "    final_clam_path = os.path.join(ac_folder, 'clam_final.csv')\n",
        "    pd.DataFrame(clam_matrix).to_csv(final_clam_path, index=False)\n",
        "    print(f\"Final CLAM matrix saved to {final_clam_path}\")\n",
        "\n",
        "    return clam_matrix\n",
        "\n",
        "# Main execution\n",
        "samples, component_list = generate_samples(n, dim, mean_vectors, covariance_matrices, weights)\n",
        "report_fractions(component_list, weights)\n",
        "visualize_samples(samples)\n",
        "save_samples(samples, component_list, samples_full_path, gt_labels_full_path)\n",
        "\n",
        "all_train_indices, all_test_indices, all_train_data, all_test_data = monte_carlo_subsampling(samples, n, B, train_num_1, save_folder)\n",
        "\n",
        "# Calculate and save sorted centroids before alignment\n",
        "sorted_centroids = calculate_and_save_sorted_centroids(samples, n_clusters_1, save_folder)\n",
        "\n",
        "# Perform agglomerative clustering\n",
        "ac_predicted_matrix = perform_ac_clustering(all_train_data, all_test_data, B, n_clusters_1, save_folder)\n",
        "\n",
        "# Align cluster identities\n",
        "aligned_matrix = align_cluster_identities(B, test_num_1, n_clusters_1, save_folder)\n",
        "\n",
        "# Generate and save CLAM matrix\n",
        "clam_matrix = generate_and_save_clam_matrix(samples, all_test_indices, aligned_matrix, save_folder, n_clusters_1)\n",
        "\n",
        "# Save run information\n",
        "run_info = {\n",
        "    'parameters': {\n",
        "        'dim': dim,\n",
        "        'n': n,\n",
        "        'variance_setting': variance_setting,\n",
        "        'covariance_type': covariance_type,\n",
        "        'B': B,\n",
        "        'n_clusters_1': n_clusters_1,\n",
        "        'percent_train': percent_train\n",
        "    },\n",
        "    'file_paths': {\n",
        "        'data_dir': data_dir,\n",
        "        'subset_indices_and_data': [\n",
        "            {\n",
        "                'subset1_indices': os.path.join(mc_subsets_dir, f'subset1_indices_{i}.csv'),\n",
        "                'subset2_indices': os.path.join(mc_subsets_dir, f'subset2_indices_{i}.csv'),\n",
        "                'subset1_data': os.path.join(mc_subsets_dir, f'subset1_data_{i}.csv'),\n",
        "                'subset2_data': os.path.join(mc_subsets_dir, f'subset2_data_{i}.csv')\n",
        "            }\n",
        "            for i in range(B)\n",
        "        ],\n",
        "        'ac_results_dir': os.path.join(save_folder, 'ac_results')\n",
        "    }\n",
        "}\n",
        "\n",
        "yaml_path = os.path.join(save_folder, 'run_info.yaml')\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(run_info, f)\n",
        "\n",
        "print(f\"Run information saved to {yaml_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "def copy_folder_to_target(src_folder, base_dest_folder):\n",
        "    # Extract the last part of the source folder's path (the folder name)\n",
        "    subfolder_name = os.path.basename(src_folder.rstrip(\"/\"))\n",
        "\n",
        "    # Construct the full destination path by appending the subfolder name to the base destination folder\n",
        "    dest_folder = os.path.join(base_dest_folder, subfolder_name)\n",
        "\n",
        "    # Ensure the destination folder exists\n",
        "    if os.path.exists(dest_folder):\n",
        "        print(f\"Destination folder {dest_folder} already exists. Removing and copying afresh.\")\n",
        "        shutil.rmtree(dest_folder)  # Remove it if it exists to avoid errors\n",
        "\n",
        "    # Copy the entire folder and its contents\n",
        "    shutil.copytree(src_folder, dest_folder)\n",
        "    print(f\"Copied entire folder {src_folder} to {dest_folder}\")\n",
        "\n",
        "# Define the source folder where your files are generated\n",
        "src_folder = save_folder  # Your save_folder variable defined above. DO NOT CHANGE!\n",
        "\n",
        "# Define the base target folder in Colab where you want to copy the files\n",
        "### Defined in the first cell!!!\n",
        "#base_dest_folder = \"/content/drive/MyDrive/Shawn_buffer_1\"  #<<********* Modify this base path as needed!!!!!!!!!!!!!!!\n",
        "\n",
        "# Copy the source folder to the target base folder, creating a subfolder with the same name as the source folder\n",
        "copy_folder_to_target(src_folder, base_dest_folder)"
      ],
      "metadata": {
        "id": "4P3yjntpiSio"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}